<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>TTS Handler Test</title>
  <style>
    body { 
      font-family: Arial, sans-serif; 
      max-width: 800px; 
      margin: 0 auto; 
      padding: 20px; 
    }
    button { 
      padding: 10px 15px; 
      margin: 5px; 
      cursor: pointer; 
    }
    textarea { 
      width: 100%; 
      height: 100px; 
      margin: 10px 0; 
    }
    pre { 
      background: #f4f4f4; 
      padding: 10px; 
      overflow: auto; 
      max-height: 300px; 
    }
    .controls { 
      display: flex; 
      gap: 10px; 
      margin-bottom: 20px; 
    }
    .log-section { 
      margin-top: 20px; 
    }
    .card {
      border: 1px solid #ddd;
      border-radius: 8px;
      padding: 15px;
      margin-bottom: 20px;
    }
    .card h3 {
      margin-top: 0;
    }
  </style>
</head>
<body>
  <h1>TTS Handler Test</h1>
  
  <div class="card">
    <h3>Audio Context</h3>
    <div class="controls">
      <button id="init-audio">Initialize Audio Context</button>
      <button id="test-audio">Test Audio Output</button>
    </div>
  </div>
  
  <div class="card">
    <h3>Send TTS Request</h3>
    <textarea id="tts-text">Hello, this is a test of the text-to-speech functionality.</textarea>
    <button id="send-tts">Send TTS Request</button>
  </div>
  
  <div class="card">
    <h3>Simulate WebRTC Messages</h3>
    <button id="send-metadata">Send Audio Metadata</button>
    <button id="send-audio">Send Audio Chunk</button>
  </div>
  
  <div class="card log-section">
    <h3>Log</h3>
    <button id="clear-log">Clear Log</button>
    <pre id="log"></pre>
  </div>
  
  <!-- Load the helper scripts -->
  <script src="/pcm_helper.js"></script>
  <script src="/tts_handler.js"></script>
  
  <script>
    // Test logger
    function log(message) {
      const logElement = document.getElementById('log');
      const timestamp = new Date().toISOString();
      logElement.textContent += `[${timestamp}] ${message}\n`;
      logElement.scrollTop = logElement.scrollHeight;
      console.log(message);
    }
    
    // Initialize audio context
    document.getElementById('init-audio').addEventListener('click', () => {
      try {
        const audioCtx = window.getAudioContext();
        log(`Audio context initialized: ${audioCtx.state}`);
        
        if (audioCtx.state !== 'running') {
          log('Resuming audio context...');
          audioCtx.resume()
            .then(() => log(`Audio context state: ${audioCtx.state}`))
            .catch(err => log(`Error resuming context: ${err.message}`));
        }
      } catch (err) {
        log(`Error initializing audio context: ${err.message}`);
      }
    });
    
    // Test audio output
    document.getElementById('test-audio').addEventListener('click', () => {
      log('Creating test audio buffer...');
      
      try {
        // First make sure audio context is initialized properly
        if (!window.audioContext && typeof window.getAudioContext === 'function') {
          window.getAudioContext();
          log('Audio context initialized');
        }
        
        // Create a direct audio test
        const AudioContext = window.AudioContext || window.webkitAudioContext;
        const testCtx = window.audioContext || new AudioContext({ sampleRate: 24000 });
        
        // Create oscillator for direct audio test
        const oscillator = testCtx.createOscillator();
        oscillator.type = 'sine';
        oscillator.frequency.setValueAtTime(440, testCtx.currentTime); // A4 note
        
        // Create gain node to control volume and prevent clicks
        const gainNode = testCtx.createGain();
        gainNode.gain.setValueAtTime(0, testCtx.currentTime);
        gainNode.gain.linearRampToValueAtTime(0.5, testCtx.currentTime + 0.05);
        gainNode.gain.linearRampToValueAtTime(0, testCtx.currentTime + 0.95);
        
        // Connect and start
        oscillator.connect(gainNode);
        gainNode.connect(testCtx.destination);
        
        oscillator.start();
        oscillator.stop(testCtx.currentTime + 1);
        
        log(`Playing direct test tone: 440Hz for 1 second`);
        
        // Also create a PCM buffer as a backup
        try {
          const sampleRate = 24000;
          const duration = 1; // 1 second
          const numSamples = sampleRate * duration;
          
          // Create Int16 buffer (for 16-bit PCM)
          const int16Buffer = new Int16Array(numSamples);
          const frequency = 880; // A5 note (one octave higher)
          
          // Generate a sine wave
          for (let i = 0; i < numSamples; i++) {
            int16Buffer[i] = Math.sin(2 * Math.PI * frequency * i / sampleRate) * 32767;
          }
          
          log(`Created PCM test audio: ${duration}s, ${frequency}Hz tone`);
          
          // Play using PCM helper after a short delay
          setTimeout(() => {
            if (typeof window.playPCMBuffer === 'function') {
              window.playPCMBuffer(int16Buffer.buffer, sampleRate, true, true)
                .then(result => log(`PCM audio playback started: ${JSON.stringify(result)}`))
                .catch(err => log(`PCM playback failed: ${err.message}`));
            } else {
              log('WARNING: playPCMBuffer function not available');
            }
          }, 1500);
        } catch (err) {
          log(`Error creating PCM test: ${err.message}`);
        }
      } catch (err) {
        log(`Error creating test audio: ${err.message}`);
      }
    });
    
    // Send TTS request
    document.getElementById('send-tts').addEventListener('click', () => {
      const text = document.getElementById('tts-text').value;
      if (!text.trim()) {
        log('Please enter some text for TTS');
        return;
      }
      
      log(`Sending TTS request for: "${text}"`);
      
      // Simulate a conversation item create message for TTS
      const message = {
        type: 'conversation.item.create',
        item: {
          content: [
            {
              type: 'input_text',
              text: text
            }
          ]
        }
      };
      
      // Process the message as if it came from WebRTC
      window.processDataChannelMessage({
        data: JSON.stringify(message)
      });
      
      // Set up timeout for audio response
      if (window.setupTTSTimeout) {
        window.setupTTSTimeout();
      }
      
      // Automatically send metadata and audio after a delay
      setTimeout(() => {
        document.getElementById('send-metadata').click();
        
        setTimeout(() => {
          document.getElementById('send-audio').click();
        }, 300);
      }, 500);
    });
    
    // Send audio metadata
    document.getElementById('send-metadata').addEventListener('click', () => {
      log('Sending audio metadata...');
      
      const metadata = {
        type: 'audio.metadata',
        format: 'int16',
        sampleRate: 24000,
        channels: 1,
        estimatedDuration: 2000 // 2 seconds
      };
      
      window.processDataChannelMessage({
        data: JSON.stringify(metadata)
      });
    });
    
    // Send audio chunk
    document.getElementById('send-audio').addEventListener('click', () => {
      log('Creating audio chunk...');
      
      try {
        // First make sure audio context is initialized
        if (typeof window.getAudioContext === 'function') {
          window.getAudioContext();
          log('Audio context initialized before sending audio chunk');
        } else {
          log('WARNING: getAudioContext function not available - audio may not play correctly');
        }
        
        // Create a simple test audio (sine wave)
        const sampleRate = 24000;
        const duration = 2; // 2 seconds
        const numSamples = sampleRate * duration;
        
        // Create Int16 buffer (for 16-bit PCM)
        const int16Buffer = new Int16Array(numSamples);
        
        // Generate a sine wave that changes frequency for more interesting audio
        let frequency = 440; // Start with A4
        for (let i = 0; i < numSamples; i++) {
          // Modulate frequency over time to create a tone that rises and falls
          const time = i / sampleRate;
          frequency = 440 + Math.sin(2 * Math.PI * 0.5 * time) * 100;
          int16Buffer[i] = Math.sin(2 * Math.PI * frequency * i / sampleRate) * 32767;
        }
        
        // Convert to base64 for transmission
        const uint8Array = new Uint8Array(int16Buffer.buffer);
        let binary = '';
        for (let i = 0; i < uint8Array.length; i++) {
          binary += String.fromCharCode(uint8Array[i]);
        }
        const base64 = btoa(binary);
        
        log(`Created audio chunk: ${uint8Array.length} bytes`);
        
        // Send as audio chunk message
        const audioChunk = {
          type: 'audio.chunk',
          chunk: {
            bytes: base64
          }
        };
        
        window.processDataChannelMessage({
          data: JSON.stringify(audioChunk)
        });
      } catch (err) {
        log(`Error creating audio chunk: ${err.message}`);
      }
    });
    
    // Clear log
    document.getElementById('clear-log').addEventListener('click', () => {
      document.getElementById('log').textContent = '';
    });
    
    // Initialize
    window.addEventListener('DOMContentLoaded', () => {
      log('TTS Handler Test page loaded');
      
      // Create a user gesture handler to initialize audio
      const initAudioOnUserGesture = () => {
        try {
          if (typeof window.getAudioContext === 'function') {
            const audioCtx = window.getAudioContext();
            log(`Audio context initialized: ${audioCtx ? audioCtx.state : 'none'}`);
            
            // Try to resume it immediately
            if (audioCtx && audioCtx.state !== 'running') {
              audioCtx.resume()
                .then(() => log(`Audio context state: ${audioCtx.state}`))
                .catch(err => log(`Error resuming context: ${err.message}`));
            }
          } else {
            log('WARNING: getAudioContext is not defined yet');
          }
        } catch (err) {
          log(`Error initializing audio: ${err.message}`);
        }
        
        // Remove the gesture handlers once used
        document.removeEventListener('click', initAudioOnUserGesture);
        document.removeEventListener('touchstart', initAudioOnUserGesture);
      };
      
      // Add handlers for the first user gesture
      document.addEventListener('click', initAudioOnUserGesture);
      document.addEventListener('touchstart', initAudioOnUserGesture);
      
      // Use a backup function if the one from tts_handler.js isn't available
      if (typeof window.getAudioContext !== 'function') {
        window.getAudioContext = function() {
          if (!window.audioContext) {
            try {
              const AudioContext = window.AudioContext || window.webkitAudioContext;
              window.audioContext = new AudioContext({ sampleRate: 24000 });
              log(`Audio context initialized (local): ${window.audioContext.state}`);
            } catch (err) {
              log(`Failed to create audio context: ${err.message}`);
            }
          }
          
          // Resume if needed
          if (window.audioContext && window.audioContext.state !== "running") {
            window.audioContext.resume()
              .then(() => log(`Audio context resumed: ${window.audioContext.state}`))
              .catch(e => log(`Failed to resume audio context: ${e.message}`));
          }
          
          return window.audioContext;
        };
        log('Added local getAudioContext function');
      }
    });
  </script>
</body>
</html>
