<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Web Audio API Test</title>
  <style>
    body { 
      font-family: Arial, sans-serif; 
      max-width: 800px; 
      margin: 0 auto; 
      padding: 20px; 
    }
    button { 
      padding: 10px 15px; 
      margin: 5px; 
      cursor: pointer; 
      background-color: #4CAF50;
      color: white;
      border: none;
      border-radius: 4px;
    }
    button:hover {
      background-color: #45a049;
    }
    pre { 
      background: #f4f4f4; 
      padding: 10px; 
      overflow: auto; 
      max-height: 300px; 
      margin-top: 20px;
    }
    .test-section {
      border: 1px solid #ddd;
      border-radius: 8px;
      padding: 15px;
      margin-bottom: 20px;
    }
  </style>
</head>
<body>
  <h1>Web Audio API Test</h1>
  
  <div class="test-section">
    <h2>Basic Audio Tests</h2>
    <p>These tests use the Web Audio API directly without any helper scripts.</p>
    
    <button id="test1">Test 1: Simple Tone</button>
    <button id="test2">Test 2: Beep Sound</button>
    <button id="test3">Test 3: Sweep</button>
  </div>

  <div class="test-section">
    <h2>PCM Audio Tests</h2>
    <p>These tests create and play PCM audio data.</p>
    
    <button id="test4">Test 4: PCM Sine Wave</button>
    <button id="test5">Test 5: PCM Sweep</button>
  </div>

  <div class="test-section">
    <h2>TTS Integration Tests</h2>
    <p>Tests for integrating with TTS handler functionality.</p>
    
    <button id="test-init-audio-ctx">Initialize Audio Context</button>
    <button id="test-audio-output">Test Audio Output</button>
    <button id="test-tts-handler">Load TTS Handler</button>
  </div>

  <div class="test-section">
    <h2>OpenAI API Tests</h2>
    <p>Test the actual OpenAI API integration for TTS and STT.</p>
    
    <div>
      <label for="tts-text">Text to speak:</label>
      <input type="text" id="tts-text" value="Hello, this is a test of the text to speech functionality." style="width: 80%; padding: 5px; margin: 5px 0;">
      <button id="test-openai-tts">Test OpenAI TTS</button>
    </div>
    
    <div style="margin-top: 15px;">
      <button id="start-recording">Start Recording</button>
      <button id="stop-recording" disabled>Stop Recording</button>
      <button id="test-openai-stt">Transcribe Recording</button>
      <div id="recording-status" style="margin-top: 5px; font-style: italic;"></div>
    </div>
  </div>
  
  <div class="test-section">
    <h2>Advanced Tests</h2>
    <button id="stop-all">Stop All Sounds</button>
    <button id="clear-log">Clear Log</button>
  </div>

  <pre id="log"></pre>

  <script>
    // Logging function
    function log(message) {
      const logElement = document.getElementById('log');
      const timestamp = new Date().toISOString();
      logElement.textContent += `[${timestamp}] ${message}\n`;
      logElement.scrollTop = logElement.scrollHeight;
      console.log(message);
    }

    // Clear log
    document.getElementById('clear-log').addEventListener('click', () => {
      document.getElementById('log').textContent = '';
    });

    // Keep track of active audio sources
    const activeSources = [];
    
    // Stop all sounds
    document.getElementById('stop-all').addEventListener('click', () => {
      log('Stopping all sounds');
      activeSources.forEach(source => {
        try {
          if (source && typeof source.stop === 'function') {
            source.stop();
          }
        } catch (err) {
          // Ignore errors from already stopped sources
        }
      });
      activeSources.length = 0;
    });

    // Audio context management
    let audioContext = null;

    // Create a local getAudioContext function that doesn't conflict with TTS handler
    function getLocalAudioContext() {
      if (!audioContext) {
        try {
          const AudioContext = window.AudioContext || window.webkitAudioContext;
          audioContext = new AudioContext();
          log(`Created audio context with state: ${audioContext.state}`);
          
          if (audioContext.state !== 'running') {
            log('Audio context is not running, attempting to resume...');
            audioContext.resume()
              .then(() => log(`Audio context resumed: ${audioContext.state}`))
              .catch(err => log(`Failed to resume audio context: ${err.message}`));
          }
        } catch (err) {
          log(`Failed to create audio context: ${err.message}`);
          return null;
        }
      }
      return audioContext;
    }
    
    // Store the original window.getAudioContext if it exists
    const originalGetAudioContext = window.getAudioContext;
    
    // Only set window.getAudioContext if it's not already defined
    if (!window.getAudioContext) {
      window.getAudioContext = getLocalAudioContext;
    }

    // Test 1: Simple Tone
    document.getElementById('test1').addEventListener('click', () => {
      log('Test 1: Playing simple tone');
      const ctx = getLocalAudioContext();
      if (!ctx) return;

      try {
        const oscillator = ctx.createOscillator();
        const gainNode = ctx.createGain();
        
        oscillator.type = 'sine';
        oscillator.frequency.setValueAtTime(440, ctx.currentTime); // 440Hz = A4
        
        gainNode.gain.setValueAtTime(0, ctx.currentTime);
        gainNode.gain.linearRampToValueAtTime(0.5, ctx.currentTime + 0.05);
        gainNode.gain.linearRampToValueAtTime(0, ctx.currentTime + 1);
        
        oscillator.connect(gainNode);
        gainNode.connect(ctx.destination);
        
        oscillator.start();
        oscillator.stop(ctx.currentTime + 1);
        
        activeSources.push(oscillator);
        
        log('Simple tone started (440Hz, 1 second)');
      } catch (err) {
        log(`Error playing simple tone: ${err.message}`);
      }
    });

    // Test 2: Beep Sound
    document.getElementById('test2').addEventListener('click', () => {
      log('Test 2: Playing beep sound');
      const ctx = getLocalAudioContext();
      if (!ctx) return;

      try {
        const oscillator = ctx.createOscillator();
        const gainNode = ctx.createGain();
        
        oscillator.type = 'square';
        oscillator.frequency.setValueAtTime(880, ctx.currentTime);
        
        gainNode.gain.setValueAtTime(0.5, ctx.currentTime);
        
        oscillator.connect(gainNode);
        gainNode.connect(ctx.destination);
        
        oscillator.start();
        oscillator.stop(ctx.currentTime + 0.2);
        
        activeSources.push(oscillator);
        
        log('Beep sound started (880Hz, 0.2 seconds)');
      } catch (err) {
        log(`Error playing beep sound: ${err.message}`);
      }
    });

    // Test 3: Sweep
    document.getElementById('test3').addEventListener('click', () => {
      log('Test 3: Playing frequency sweep');
      const ctx = getLocalAudioContext();
      if (!ctx) return;

      try {
        const oscillator = ctx.createOscillator();
        const gainNode = ctx.createGain();
        
        oscillator.type = 'sine';
        oscillator.frequency.setValueAtTime(220, ctx.currentTime);
        oscillator.frequency.linearRampToValueAtTime(880, ctx.currentTime + 2);
        
        gainNode.gain.setValueAtTime(0, ctx.currentTime);
        gainNode.gain.linearRampToValueAtTime(0.5, ctx.currentTime + 0.05);
        gainNode.gain.linearRampToValueAtTime(0, ctx.currentTime + 2);
        
        oscillator.connect(gainNode);
        gainNode.connect(ctx.destination);
        
        oscillator.start();
        oscillator.stop(ctx.currentTime + 2);
        
        activeSources.push(oscillator);
        
        log('Frequency sweep started (220Hz to 880Hz, 2 seconds)');
      } catch (err) {
        log(`Error playing frequency sweep: ${err.message}`);
      }
    });

    // Test 4: PCM Sine Wave
    document.getElementById('test4').addEventListener('click', () => {
      log('Test 4: Playing PCM sine wave');
      const ctx = getLocalAudioContext();
      if (!ctx) return;

      try {
        const sampleRate = ctx.sampleRate;
        const duration = 1; // 1 second
        const numSamples = sampleRate * duration;
        
        // Create audio buffer
        const audioBuffer = ctx.createBuffer(1, numSamples, sampleRate);
        const channelData = audioBuffer.getChannelData(0);
        
        // Fill with sine wave
        const frequency = 440;
        for (let i = 0; i < numSamples; i++) {
          channelData[i] = Math.sin(2 * Math.PI * frequency * i / sampleRate);
        }
        
        // Play buffer
        const source = ctx.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(ctx.destination);
        source.start();
        
        activeSources.push(source);
        
        log(`PCM sine wave started (${frequency}Hz, ${duration}s, ${sampleRate}Hz sample rate)`);
      } catch (err) {
        log(`Error playing PCM sine wave: ${err.message}`);
      }
    });

    // Test 5: PCM Sweep
    document.getElementById('test5').addEventListener('click', () => {
      log('Test 5: Playing PCM frequency sweep');
      const ctx = getLocalAudioContext();
      if (!ctx) return;

      try {
        const sampleRate = ctx.sampleRate;
        const duration = 2; // 2 seconds
        const numSamples = sampleRate * duration;
        
        // Create audio buffer
        const audioBuffer = ctx.createBuffer(1, numSamples, sampleRate);
        const channelData = audioBuffer.getChannelData(0);
        
        // Fill with frequency sweep
        const startFreq = 220;
        const endFreq = 880;
        
        for (let i = 0; i < numSamples; i++) {
          // Calculate frequency at this point in time
          const t = i / sampleRate;
          const freqT = startFreq + (endFreq - startFreq) * (t / duration);
          
          // Need to calculate phase by integrating frequency over time
          const phase = 2 * Math.PI * (startFreq * t + (endFreq - startFreq) * t * t / (2 * duration));
          
          // Apply amplitude envelope
          const amplitude = Math.min(1, Math.min(t * 10, (duration - t) * 10));
          
          channelData[i] = Math.sin(phase) * amplitude;
        }
        
        // Play buffer
        const source = ctx.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(ctx.destination);
        source.start();
        
        activeSources.push(source);
        
        log(`PCM frequency sweep started (${startFreq}Hz to ${endFreq}Hz, ${duration}s)`);
      } catch (err) {
        log(`Error playing PCM frequency sweep: ${err.message}`);
      }
    });

    // TTS Integration Tests
    document.getElementById('test-init-audio-ctx').addEventListener('click', () => {
      log('Testing Audio Context initialization');
      const ctx = getLocalAudioContext();
      
      if (ctx) {
        log(`Audio context successfully created: sampleRate=${ctx.sampleRate}, state=${ctx.state}`);
        
        // Play a short beep to verify audio is working
        const oscillator = ctx.createOscillator();
        const gainNode = ctx.createGain();
        
        oscillator.type = 'sine';
        oscillator.frequency.value = 440;
        
        gainNode.gain.value = 0.2;
        
        oscillator.connect(gainNode);
        gainNode.connect(ctx.destination);
        
        oscillator.start();
        oscillator.stop(ctx.currentTime + 0.3);
        
        log('Audio context verification tone played');
      } else {
        log('Failed to create audio context');
      }
    });
    
    document.getElementById('test-audio-output').addEventListener('click', () => {
      log('Testing audio output with simple PCM buffer');
      const ctx = getLocalAudioContext();
      if (!ctx) {
        log('No audio context available. Click "Initialize Audio Context" first');
        return;
      }
      
      try {
        // Create a sample 24kHz PCM buffer (similar to what TTS might generate)
        const sampleRate = 24000;
        const duration = 0.5;
        const numSamples = Math.floor(sampleRate * duration);
        
        // Create 16-bit PCM data (Int16Array)
        const pcmData = new Int16Array(numSamples);
        
        // Fill with a simple tone
        const frequency = 440;
        for (let i = 0; i < numSamples; i++) {
          // Generate sine wave and scale to 16-bit range
          pcmData[i] = Math.sin(2 * Math.PI * frequency * i / sampleRate) * 32767;
        }
        
        // Convert to Float32Array (similar to what pcmHelper would do)
        const floatData = new Float32Array(pcmData.length);
        for (let i = 0; i < pcmData.length; i++) {
          floatData[i] = pcmData[i] / 32768.0;
        }
        
        // Create audio buffer
        const audioBuffer = ctx.createBuffer(1, floatData.length, sampleRate);
        const channelData = audioBuffer.getChannelData(0);
        
        // Copy the float data
        channelData.set(floatData);
        
        // Play the buffer
        const source = ctx.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(ctx.destination);
        source.start();
        
        activeSources.push(source);
        
        log(`PCM audio test played (${sampleRate}Hz sample rate, ${duration}s duration)`);
      } catch (err) {
        log(`Error testing PCM audio: ${err.message}`);
      }
    });
    
    document.getElementById('test-tts-handler').addEventListener('click', () => {
      log('Loading TTS handler scripts');
      
      // Before loading the TTS scripts, make sure we're not going to cause 
      // conflicts with our audio context
      
      // Save our local audio context for later reuse
      const localAudioCtx = getLocalAudioContext();
      if (localAudioCtx) {
        log(`Using existing audio context with sample rate: ${localAudioCtx.sampleRate}`);
      }
      
      // Function to load external script
      const loadScript = (url) => {
        return new Promise((resolve, reject) => {
          const script = document.createElement('script');
          script.src = url;
          script.onload = () => {
            log(`Successfully loaded ${url}`);
            resolve();
          };
          script.onerror = () => {
            const errorMsg = `Failed to load script: ${url}`;
            log(errorMsg);
            reject(new Error(errorMsg));
          };
          document.head.appendChild(script);
        });
      };
      
      // Load the pcm_helper.js first
      loadScript('./pcm_helper.js')
        .then(() => {
          // Now that PCM helper is loaded, we can safely load TTS handler
          return loadScript('./tts_handler.js');
        })
        .then(() => {
          log('All TTS scripts loaded successfully');
          
          // Verify window functions are available
          if (typeof window.getAudioContext === 'function') {
            log('✓ window.getAudioContext is available');
          } else {
            log('✗ window.getAudioContext is NOT available');
          }
          
          if (typeof window.processDataChannelMessage === 'function') {
            log('✓ window.processDataChannelMessage is available');
          } else {
            log('✗ window.processDataChannelMessage is NOT available');
          }
          
          if (typeof window.playPCMBuffer === 'function') {
            log('✓ playPCMBuffer is available');
          } else {
            log('✗ playPCMBuffer is NOT available');
          }
          
          log('TTS handler initialized successfully. Sound buttons should continue to work.');
        })
        .catch(err => {
          log(`Error initializing TTS handler: ${err.message}`);
        });
    });

    // OpenAI TTS Test
    document.getElementById('test-openai-tts').addEventListener('click', () => {
      const text = document.getElementById('tts-text').value.trim();
      if (!text) {
        log('Please enter text to synthesize speech');
        return;
      }
      
      log(`Testing OpenAI TTS with text: "${text}"`);
      
      // Make sure TTS handler is loaded
      if (!window.processDataChannelMessage) {
        log('TTS handler not loaded. Click "Load TTS Handler" first');
        return;
      }
      
      // Create a mock data channel message with the text
      const mockMessage = {
        data: JSON.stringify({
          type: 'conversation.item.create',
          item: {
            content: [
              {
                type: 'input_text',
                text: text
              }
            ]
          }
        })
      };
      
      log('Sending mock TTS request');
      
      // Call the API through fetch
      fetch('/api/tts', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ text: text })
      })
      .then(response => {
        if (!response.ok) {
          throw new Error(`TTS API returned ${response.status}: ${response.statusText}`);
        }
        return response.arrayBuffer();
      })
      .then(audioData => {
        log(`Received TTS audio data: ${audioData.byteLength} bytes`);
        
        // Create an audio blob from the MP3 data
        const audioBlob = new Blob([audioData], { type: 'audio/mp3' });
        const audioUrl = URL.createObjectURL(audioBlob);
        
        // Create and play audio element
        const audioElement = new Audio(audioUrl);
        
        audioElement.onloadedmetadata = () => {
          log(`MP3 audio loaded, duration: ${audioElement.duration.toFixed(2)} seconds`);
        };
        
        audioElement.onplay = () => {
          log('MP3 audio playback started');
        };
        
        audioElement.onended = () => {
          log('MP3 audio playback completed');
          // Clean up the object URL
          URL.revokeObjectURL(audioUrl);
        };
        
        audioElement.onerror = (err) => {
          log(`Error playing MP3 audio: ${err}`);
        };
        
        // Play the audio
        audioElement.play()
          .then(() => {
            log('Audio playback initiated');
          })
          .catch(err => {
            log(`Failed to start audio playback: ${err.message}`);
          });
      })
      .catch(error => {
        log(`Error with TTS request: ${error.message}`);
      });
    });
    
    // Speech-to-Text functionality
    let mediaRecorder = null;
    let audioChunks = [];
    let recordingStream = null;
    
    // Start recording
    document.getElementById('start-recording').addEventListener('click', () => {
      // Reset previous recording if any
      audioChunks = [];
      
      log('Requesting microphone access...');
      
      navigator.mediaDevices.getUserMedia({ audio: true, video: false })
        .then(stream => {
          recordingStream = stream;
          document.getElementById('recording-status').textContent = 'Recording...';
          document.getElementById('start-recording').disabled = true;
          document.getElementById('stop-recording').disabled = false;
          
          log('Microphone access granted, starting recording');
          
          mediaRecorder = new MediaRecorder(stream);
          
          mediaRecorder.ondataavailable = event => {
            if (event.data.size > 0) {
              audioChunks.push(event.data);
            }
          };
          
          mediaRecorder.onstop = () => {
            log(`Recording stopped: ${audioChunks.length} chunks collected`);
          };
          
          mediaRecorder.start(100);
        })
        .catch(error => {
          log(`Microphone access error: ${error.message}`);
        });
    });
    
    // Stop recording
    document.getElementById('stop-recording').addEventListener('click', () => {
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        mediaRecorder.stop();
        document.getElementById('recording-status').textContent = 'Recording stopped';
        log('Recording stopped');
      }
      
      if (recordingStream) {
        recordingStream.getTracks().forEach(track => track.stop());
      }
      
      document.getElementById('start-recording').disabled = false;
      document.getElementById('stop-recording').disabled = true;
    });
    
    // Transcribe recording
    document.getElementById('test-openai-stt').addEventListener('click', () => {
      if (audioChunks.length === 0) {
        log('No recording available. Please record audio first.');
        return;
      }
      
      log('Preparing audio for transcription...');
      
      const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
      const formData = new FormData();
      formData.append('file', audioBlob, 'recording.webm');
      
      log(`Sending audio for transcription (${audioBlob.size} bytes)`);
      
      // Call the STT API through fetch
      fetch('/api/stt', {
        method: 'POST',
        body: formData
      })
      .then(response => {
        if (!response.ok) {
          throw new Error(`STT API returned ${response.status}: ${response.statusText}`);
        }
        return response.json();
      })
      .then(data => {
        log(`Transcription result: "${data.text}"`);
      })
      .catch(error => {
        log(`Error with STT request: ${error.message}`);
      });
    });

    // Initialize
    window.addEventListener('DOMContentLoaded', () => {
      log('Audio test page loaded');
      log('Click any test button to begin');
      log('Note: Some browsers require user interaction before audio can play');
    });
  </script>
</body>
</html>
